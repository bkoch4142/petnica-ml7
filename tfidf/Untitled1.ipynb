{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "verified-voltage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chines, goos, breed, gees, the, knob, swan, of, femal, wild\n",
      "2.148510324599202\n",
      "1.8504567133793062\n",
      "1.2866018496400775\n",
      "0.8421373012398391\n",
      "2.0953112414358794\n",
      "1.6130756409113107\n",
      "1.8460688892352195\n",
      "1.642651057586106\n",
      "2.215718724220454\n",
      "[('A female Chinese goose can lay 50–60 eggs over the course of the breeding season (February to June), although there are reports of Chinese geese laying up to 100 eggs during that time.', 2.215718724220454), ('The Chinese goose is a breed of domesticated goose descended from the wild swan goose.', 2.148510324599202), ('Chinese geese differ from the wild birds in much larger size (up to 5–10 kg in males, 4–9 kg in females), and in having an often strongly developed basal knob on the upper side of the bill.', 1.8504567133793062), ('Chinese geese are a close cousin of the African goose, a heavier breed also descended from the swan goose.', 2.0953112414358794), (\"While many domestic Chinese geese have a similar body type to other breeds, the breed standards as defined in the American Poultry Association's Standard of Perfection and other sources call for a slimmer, taller fowl.\", 1.8460688892352195)]\n",
      "\n",
      "Wall time: 4.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import math\n",
    "import nltk\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def preprocessor(txt):\n",
    "    tokens=[stemmer.stem(token.lower()) for token in tokenizer(txt) if token.isalnum()]\n",
    "    return tokens\n",
    "\n",
    "def tfidf(tok_doc, tok_corpus):\n",
    "    doc_length=len(tok_doc)\n",
    "    corpus_length=len(tok_corpus)\n",
    "\n",
    "    #tf\n",
    "    term_count=Counter(tok_doc)\n",
    "\n",
    "    #idf\n",
    "    tfidf={}\n",
    "    for key in tok_doc:\n",
    "        doc_frequency=0\n",
    "        for doc in tok_corpus:\n",
    "            if key in doc:\n",
    "                doc_frequency+=1\n",
    "        inverse_doc_frequency=math.log(corpus_length/doc_frequency)\n",
    "\n",
    "        tfidf[key]= (term_count[key]/doc_length) * inverse_doc_frequency\n",
    "\n",
    "    return tfidf\n",
    "\n",
    "def task1(tok_text, tfidf_scores):\n",
    "\n",
    "    # extract top 10\n",
    "    top_10=[(k,v) for k, v in sorted(tfidf_scores.items(), key=lambda item: item[1], reverse=True)][:10]\n",
    "\n",
    "    #print top 10\n",
    "    print(', '.join([k for k,v in top_10]))\n",
    "\n",
    "%timeit\n",
    "def task2(text,tfidf_scores):\n",
    "    sentences=sent_tokenize(text)\n",
    "    if len(sentences)<6:\n",
    "        return text # da li treba spojit s razmakom ili samo text vratit \n",
    "\n",
    "    sent_scores=[]\n",
    "    tok_text=preprocessor(text)\n",
    "\n",
    "    for sent in sentences:\n",
    "        tok_sent=preprocessor(sent)\n",
    "        tfidfs=[tfidf_scores[tok] for tok in tok_sent]\n",
    "        tfidfs=np.array(tfidfs)\n",
    "\n",
    "        if len(tfidfs)<=10:\n",
    "            sent_score=sum(tfidfs)\n",
    "        else:\n",
    "            sent_score= np.sum(-(np.partition(-tfidfs, 10))[:10])\n",
    "        print(sent_score)\n",
    "\n",
    "\n",
    "        sent_scores.append((sent,sent_score))\n",
    "\n",
    "    sentences_result=[]\n",
    "#         for sent,val in sorted(sent_scores, key=lambda item: item[1], reverse=True)[:5]:\n",
    "#             sentences_result.append(sent)\n",
    "    if len(sent_scores)>5:\n",
    "        vals=np.array([val for sent,val in sent_scores])\n",
    "        for idx in ((np.argpartition(-vals, 5))[:5]):\n",
    "            sentences_result.append(sent_scores[idx])\n",
    "    else:\n",
    "        for sent,val in sent_scores:\n",
    "            sentences_result.append(sent)\n",
    "    print(sentences_result)\n",
    "\n",
    "    sentences_sorted=[]\n",
    "    for sent in sentences:\n",
    "        if sent in sentences_result:\n",
    "            sentences_sorted.append(sent)\n",
    "\n",
    "    print(' '.join(sentences_sorted))\n",
    "\n",
    "\n",
    "# setup \n",
    "corpus_pth='data\\\\corpus'\n",
    "input_pth='data\\\\corpus\\\\goose\\\\Chinese goose.txt'\n",
    "\n",
    "tokenizer=word_tokenize\n",
    "stemmer=SnowballStemmer(language='english', ignore_stopwords=False)\n",
    "\n",
    "corpus_string=''\n",
    "for root, dirs, files in os.walk(corpus_pth, topdown=False):\n",
    "   for name in files:\n",
    "      corpus_string+=str(\n",
    "              open(os.path.join(root, name),'r', encoding='UTF-8').read()\n",
    "        )\n",
    "\n",
    "tok_corpus=preprocessor(corpus_string)\n",
    "\n",
    "input_text=open(input_pth,'r', encoding='UTF-8').read()\n",
    "tok_text=preprocessor(input_text)\n",
    "tfidf_scores=tfidf(tok_text,tok_corpus)\n",
    "\n",
    "# task 1\n",
    "task1(tok_text, tfidf_scores)\n",
    "\n",
    "# task 2\n",
    "task2(input_text, tfidf_scores)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-serial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "british-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "documented-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad=np.array([4,7,2,8,4,99,22,3,5,2643,3])\n",
    "five=np.partition(-ad, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "tough-elevation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -99, -2643,   -22,    -8,    -7,    -5,    -4,    -3,    -4,\n",
       "          -2,    -3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "loose-penguin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2779"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(-five[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "numeric-flood",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-2cdbb761adcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidfs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidfs' is not defined"
     ]
    }
   ],
   "source": [
    "tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eight-springfield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 11,  5], dtype=int64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfs=np.array([4,7,2,8,4,99,22,3,5,54,325,623])\n",
    "(np.argpartition(-tfidfs, 3))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "accepting-village",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfs[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "equipped-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 11  5  9  6]\n"
     ]
    }
   ],
   "source": [
    "items=np.array([4,7,2,8,4,99,22,3,5,54,325,623])\n",
    "\n",
    "top_10_idxs=np.argpartition(-items, 5)[:5]\n",
    "print(top_10_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "worldwide-specification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-aircraft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
